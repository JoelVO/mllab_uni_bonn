{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4f0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:585: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object:\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool:\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/util/nest.py:823: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  _pywrap_tensorflow.RegisterType(\"Mapping\", _collections.Mapping)\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:312: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class _ListWrapper(List, collections.MutableSequence,\n",
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class _ObjectIdentitySet(collections.MutableSet):\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaaf7d",
   "metadata": {},
   "source": [
    "## 6.1. Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3ce3e",
   "metadata": {},
   "source": [
    "For the problem formulation, we introduce the [gym](https://www.gymlibrary.ml/) library. It implements control problems from the past and present of reinforcement learning that have served as milestones in the development of that technique. Researchers that work on the same standard problems have the advantage that their work is easier to compare and to transfer. On the other hand, if benchmark problems are too prevalent in a community, it may drive research in a certain, uniform direction that is not as productive anymore. Note that gym is a product of OpenAI, a private company. \n",
    "\n",
    "gym uses a unifying framework that defines every control problem as an *environment*. The basic building blocks of an environment are `env = gym.make` to create the environment, `env.reset` to start an episode, `env.render` to give a human readable representation of the state of the environment, and `env.step` to perform an action.\n",
    "\n",
    "We start the exercises with the 4x4 [FrozenLake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) environment. It is a kind of maze with \"frozen\" traversable squares marked by `F` and \"holes\", losing terminal squares marked by `H`. The agent starts at the `S` start square and only incurs reward, when they manage to get to the goal `G` square. We mostly look at the deterministic case, where traversing on the frozen lake is deterministic, which is controlled by the variable `is_slippery=False` when creating the environment. If the lake is slippery, a movement in a certain direction may by chance result in the agent arriving at a different square than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3ef494",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "#print(env.action_space)\n",
    "#print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0af69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = env.reset()\n",
    "#print(starting_state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34850d84",
   "metadata": {},
   "source": [
    "The `env.action_space` always implements a `sample` method, which returns a valid, random aciton. We can utilize this, to have a look at the dynamics of the system. You can execute the following cell a few times to see what happens. When the agent enters a terminal state, you need to execute `env.reset` to start anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47af7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.0 False {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(env.action_space.sample())\n",
    "print(state, reward, done, info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb143e",
   "metadata": {},
   "source": [
    "#### Task 1. a) Random Agent:\n",
    "We provide the framework for the random agent, a method to rollout a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb60f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent, render=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "    return total_reward\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "    # We pass the state only for compatability\n",
    "    def action(self, state):\n",
    "        # TODO implement here\n",
    "        return np.random.choice(self.action_space.n)\n",
    "\n",
    "def compute_avg_return(env, agent, f_rollout = rollout,num_episodes=5000):\n",
    "    # TODO implement here\n",
    "    rewards = []\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        episode_reward = f_rollout(env,agent)\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "    avg_reward = np.mean(rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85a08a",
   "metadata": {},
   "source": [
    "Add your code to estimate the `avg_return_random_agent` for the deterministic case and `avg_return_random_agent_slippery` for the stochastic case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1754c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5000/5000 [00:01<00:00, 3831.76it/s]\n",
      "100%|█████████████████████████████████████| 5000/5000 [00:01<00:00, 3448.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO implement here\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "avg_return_random_agent = compute_avg_return(env,RandomAgent(env.action_space,env.observation_space))\n",
    "\n",
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "avg_return_random_agent_slippery = compute_avg_return(env,RandomAgent(env.action_space,env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec8e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation for the deterministic case: 0.0134\n",
      "Estimation for the stochastic case: 0.0156\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimation for the deterministic case:\", avg_return_random_agent)\n",
    "print(\"Estimation for the stochastic case:\", avg_return_random_agent_slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585ca41",
   "metadata": {},
   "source": [
    "The expected value goes smaller when we consider is_slippery = True because it introduces some type of noise on the possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c63c3",
   "metadata": {},
   "source": [
    "### 1. b) Iterative Policy Evaluation\n",
    "We provide a `set_state` method that changes the state of the environment. This is a pretty unusual way to interact with this framework. Note, that the random policy is stochastic, while the environment is not. In the value update we sum the value of each possible action that is weighted by its probability to be picked by the action. The architecture of the agent does provide access to these inner dynamics, so instead of passing the agent or its dynamics as a variable, we implement iterative policy evaluation just for the random agent, with the probability of `0.25` for each action hard coded.\n",
    "\n",
    "We also provide `all_states` and `all_actions`, lists of all admissable states and actions for the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ce56d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = list(range(env.observation_space.n))\n",
    "all_actions = list(range(env.action_space.n))\n",
    "\n",
    "def set_state(env, state):\n",
    "    env.reset()\n",
    "    env.env.env.env.s = state\n",
    "    return env\n",
    "\n",
    "def visualize_value_fct(v):\n",
    "    print(np.round(np.array(list(v.values())).reshape((4,4)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5a32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate, \n",
    "                                            threshold=0.001, max_iter=10000):\n",
    "    v = {s: 0 for s in all_states}  # value function, initialized to 0\n",
    "    it = 0\n",
    "    while it < max_iter:\n",
    "        v1 = {s: v[s] for s in all_states}\n",
    "        for s in all_states:\n",
    "            reward = 0\n",
    "            for a in all_actions:\n",
    "                env = set_state(env,s)\n",
    "                state, reward1, done, info = env.step(a)\n",
    "                reward += 0.25*(reward1 + discount_rate*v1[state])\n",
    "            v[s] = reward \n",
    "        if np.amax(np.abs(np.array(list(v.values())) - np.array(list(v1.values())))) <= threshold:\n",
    "            break\n",
    "        it += 1\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35557fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.003 0.003 0.009 0.003]\n",
      " [0.006 0.    0.026 0.   ]\n",
      " [0.018 0.056 0.106 0.   ]\n",
      " [0.    0.129 0.39  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "v_random = iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9619a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.012 0.008 0.03  0.002]\n",
      " [0.022 0.    0.021 0.   ]\n",
      " [0.022 0.052 0.072 0.   ]\n",
      " [0.    0.061 0.382 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "v_random = iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00941b0",
   "metadata": {},
   "source": [
    "### 1. c) Value Iteration\n",
    "Use value iteration to find the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec0d8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.001\n",
    "discount_rate = 0.9\n",
    "\n",
    "v1 = {s: 0 for s in all_states}\n",
    "v = {s: np.random.uniform(0,10) for s in all_states}\n",
    "count = 0\n",
    "while np.amax(np.abs(np.array(list(v.values())) - np.array(list(v1.values())))) > threshold:\n",
    "    v = {s: v1[s] for s in all_states}\n",
    "    for s in all_states:\n",
    "        rewards = []\n",
    "        for a in all_actions:\n",
    "            env = set_state(env,s)\n",
    "            state, reward1, done, info = env.step(a)\n",
    "            rewards.append(reward1 + discount_rate*v[state])\n",
    "            \n",
    "        v1[s] = np.amax(rewards)\n",
    "        #print(np.amax(rewards))\n",
    "        \n",
    "    count += 1\n",
    "    if count > 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61587995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, all_states, all_actions, discount_rate, threshold=0.001, max_iter=10000):\n",
    "    # TODO implement here\n",
    "    v1 = {s: 0 for s in all_states}\n",
    "    v = {s: np.random.uniform(0,10) for s in all_states}\n",
    "    steps = {s: -1 for s in all_states}\n",
    "    for it in range(max_iter):\n",
    "        v = {s: v1[s] for s in all_states}\n",
    "        for s in all_states:\n",
    "            rewards = []\n",
    "            for a in all_actions:\n",
    "                env = set_state(env,s)\n",
    "                state, reward1, done, info = env.step(a)\n",
    "                rewards.append(reward1 + discount_rate*v[state])\n",
    "\n",
    "            v1[s] = np.amax(rewards)\n",
    "            steps[s] = np.argmax(rewards)\n",
    "\n",
    "        if np.amax(np.abs(np.array(list(v.values())) - np.array(list(v1.values())))) <= threshold:\n",
    "            break\n",
    "    return v,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d68d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59  0.656 0.729 0.656]\n",
      " [0.656 0.    0.81  0.   ]\n",
      " [0.729 0.81  0.9   0.   ]\n",
      " [0.    0.9   1.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "v_optimal,policy_optimal = value_iteration(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "840e319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_reward(env,policy_optimal):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    state = 0\n",
    "\n",
    "    while not done:\n",
    "        state, reward1, done, info = env.step(policy_optimal[state])\n",
    "        reward += reward1\n",
    "        \n",
    "    return reward\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df29c7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal reward is  1.0\n"
     ]
    }
   ],
   "source": [
    "print('The optimal reward is ',optimal_reward(env,policy_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5617c",
   "metadata": {},
   "source": [
    "### 2. a) Sarsa & Q-Learning\n",
    "With the language of a Q-table, we can define a more general agent by a Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc7e6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_fct(q):\n",
    "    acts = {0 : \"L\", 1 : \"D\", 2 : \"R\", 3 : \"U\"} \n",
    "    for j in range(4):\n",
    "        print(\"Value for action\", acts[j], \":\")\n",
    "        print(np.round(np.array([q[i][j] for i in range(16)]).reshape((4,4)), 3))\n",
    "    for i in range(4):\n",
    "        print([acts[np.argmax(q[4*i + j])] for j in range(4)])\n",
    "        \n",
    "def argmax_tiebreak(array):\n",
    "    return np.random.choice(np.where(array == array.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "904ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete_Q_Agent:\n",
    "    def __init__(self,env, action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.reset_Q()\n",
    "        self.env = env\n",
    "    def reset_Q(self):\n",
    "        all_states = list(range(self.observation_space.n))\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        self.Q = {s: np.zeros(self.action_space.n) for s in all_states}\n",
    "\n",
    "    def action(self, state): #made to solve SARSA\n",
    "        # TODO implement here\n",
    "        env = self.env\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(list(range(self.action_space.n)))\n",
    "        else:\n",
    "            #rewards = []\n",
    "            #for a in list(range(self.action_space.n)):\n",
    "            #    env = set_state(env,state)\n",
    "            #    state, reward1, done, info = env.step(a)\n",
    "            #    rewards.append(reward1)\n",
    "            #action = list(range(self.action_space.n))[np.argmax(rewards)]\n",
    "            \n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88b55655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    # TODO implement here\n",
    "    q_agent.reset_Q()\n",
    "    for rollout in range(rollouts):\n",
    "        #state = np.random.choice(list(range(q_agent.observation_space.n)))\n",
    "        state = env.reset()\n",
    "        action = q_agent.action(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            state1, r, done, info = env.step(action)\n",
    "            action1 = q_agent.action(state1)\n",
    "            q_agent.Q[state][action] = q_agent.Q[state][action] + alpha*(r + gamma*q_agent.Q[state1][action1] - q_agent.Q[state][action]) \n",
    "            \n",
    "            state,action = state1,action1\n",
    "         \n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bfadf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    # TODO implement here\n",
    "    q_agent.reset_Q()\n",
    "    for rollout in range(rollouts):\n",
    "        state = np.random.choice(list(range(q_agent.observation_space.n)))\n",
    "        action = q_agent.action(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            if np.random.uniform(0,1) < q_agent.epsilon:\n",
    "                action1 = np.random.choice(q_agent.action_space.n)\n",
    "                env = set_state(env,state)\n",
    "                state1, r, done, info = env.step(action1)\n",
    "                q_agent.Q[state][action] += alpha*(r + gamma*q_agent.Q[state1][action1] - q_agent.Q[state][action]) \n",
    "            \n",
    "                \n",
    "            else:\n",
    "                temp_diff,rew = [],[]\n",
    "                for a in range(q_agent.action_space.n):\n",
    "                    state1, r, done, info = env.step(a)\n",
    "                    temp_diff.append(q_agent.Q[state1][a] - q_agent.Q[state][action])\n",
    "                    rew.append(r)\n",
    "\n",
    "                r,action1 = rew[np.argmax(temp_diff)],np.argmax(temp_diff)\n",
    "                temporal_difference = np.amax(temp_diff)\n",
    "                q_agent.Q[state][action] += alpha*(r + gamma*temporal_difference) \n",
    "\n",
    "                env = set_state(env,state)\n",
    "                state1, r, done, info = env.step(action1)\n",
    "\n",
    "            state,action = state1,action1\n",
    "                \n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7993db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for action L :\n",
      "[[0.045 0.024 0.056 0.015]\n",
      " [0.078 0.    0.04  0.   ]\n",
      " [0.057 0.117 0.311 0.   ]\n",
      " [0.    0.162 0.349 0.   ]]\n",
      "Value for action D :\n",
      "[[0.042 0.027 0.045 0.023]\n",
      " [0.04  0.    0.05  0.   ]\n",
      " [0.077 0.218 0.185 0.   ]\n",
      " [0.    0.21  0.521 0.   ]]\n",
      "Value for action R :\n",
      "[[0.051 0.02  0.04  0.019]\n",
      " [0.039 0.    0.098 0.   ]\n",
      " [0.084 0.173 0.164 0.   ]\n",
      " [0.    0.299 0.44  0.   ]]\n",
      "Value for action U :\n",
      "[[0.041 0.04  0.03  0.04 ]\n",
      " [0.037 0.    0.011 0.   ]\n",
      " [0.112 0.104 0.052 0.   ]\n",
      " [0.    0.248 0.341 0.   ]]\n",
      "['R', 'U', 'L', 'U']\n",
      "['L', 'L', 'R', 'L']\n",
      "['U', 'D', 'L', 'L']\n",
      "['L', 'R', 'D', 'L']\n"
     ]
    }
   ],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery,env_slippery.action_space, env_slippery.observation_space, epsilon=0.1)\n",
    "q_agent, q = Sarsa(env_slippery, q_agent,gamma = 0.9)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91535fe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Cannot call env.step() before calling env.reset()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResetNeeded\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36448/1655171881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv_slippery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FrozenLake-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscrete_Q_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_slippery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv_slippery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_slippery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_Learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_slippery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvisualize_q_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36448/3191717765.py\u001b[0m in \u001b[0;36mQ_Learning\u001b[0;34m(env, q_agent, alpha, gamma, rollouts)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mtemp_diff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0mtemp_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mrew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"TimeLimit.truncated\"\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Steps through the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResetNeeded\u001b[0m: Cannot call env.step() before calling env.reset()"
     ]
    }
   ],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery,env_slippery.action_space, env_slippery.observation_space, epsilon=0.1)\n",
    "q_agent, q = Q_Learning(env_slippery, q_agent,gamma = 0.9)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4eab0e",
   "metadata": {},
   "source": [
    "### 2. b) Cartpole\n",
    "Next, try the [Cartpole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) environment. It has a continuous state space, so we need to adjust our methods to accomodate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eaaac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "#!pip install piglet\n",
    "#!sudo apt-get install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03c25466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_cartpole(env, agent, render=False):\n",
    "    env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return env.file_infix\n",
    "    \n",
    "def rollout_cart_avg(env, agent, render=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95f036dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:98: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n",
      "100%|█████████████████████████████████████| 5000/5000 [00:02<00:00, 1757.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return for a random strategy in cartpole: 22.0164\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36448/3970195876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_return_random_cartpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average return for a random strategy in cartpole:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return_random_cartpole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile_infix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gym-results/openaigym.video.%s.video000000.mp4'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_infix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36448/415959605.py\u001b[0m in \u001b[0;36mrollout_cartpole\u001b[0;34m(env, agent, render)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrollout_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./gym-results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
     ]
    }
   ],
   "source": [
    "# TODO implement here\n",
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "random_agent = RandomAgent(cartpole.action_space,cartpole.observation_space)\n",
    "avg_return_random_cartpole = compute_avg_return(cartpole,RandomAgent(cartpole.action_space,cartpole.observation_space))\n",
    "print(\"Average return for a random strategy in cartpole:\", avg_return_random_cartpole)\n",
    "file_infix = rollout_cartpole(cartpole,random_agent)\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79b2d8",
   "metadata": {},
   "source": [
    "### 2. c) Cartpole learning\n",
    "The observation space of the Cartpole environment can be accessed with `env.observation_space`. It is a [`Box`](https://www.gymlibrary.ml/content/spaces/#box) space, which contains lower bounds, upper bounds, number of dimensions, and datatype. The second and forth dimension are unbounded. We can make them bounded by clipping every value over a certain threshold. Also, the first and third dimension have higher admissbable bounds, than is useful during training!\n",
    "\n",
    "Hint: Binned Q-Learning is not the most efficient or useful algorithm for this problem. With the provided hyperparameters I achieved only a mean reward of ~100 after 50000 rollouts of training without any further tuning. Can you achieve a better result by changing the hyperparameters or employing some additional technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b052a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "discounting_rate = 0.95\n",
    "number_episodes = 50000\n",
    "total_reward = 0\n",
    "\n",
    "window_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "low_clip = np.array([-2.4, -3.75, -0.2095, -2.5])\n",
    "high_clip = np.array([2.4, 3.75, 0.2095, 2.5])\n",
    "\n",
    "class Binned_Q_Agent_Cartpole:\n",
    "    def __init__(self, env,window_size,low_clip,high_clip):\n",
    "        # TODO implement here\n",
    "        self.env = env\n",
    "        self.window_size = window_size\n",
    "        self.low_clip = low_clip\n",
    "        self.high_clip = high_clip\n",
    "        self.q_table = np.zeros(np.append(np.asarray((high_clip - low_clip)/window_size,dtype = 'int') +1,2))\n",
    "\n",
    "\n",
    "    def get_discrete_state(self, state):\n",
    "        # TODO implement here\n",
    "        state = np.maximum(state,self.low_clip)\n",
    "        state = np.minimum(state,self.high_clip)\n",
    "\n",
    "        return np.asarray((state - self.low_clip)/window_size,dtype = 'int')\n",
    "    \n",
    "\n",
    "    \n",
    "    def action(self, state,epsilon = 0.05):\n",
    "        # TODO implement here\n",
    "        env = self.env\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            action = np.random.choice(list(range(env.action_space.n)))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            disc_state = self.get_discrete_state(state)\n",
    "            action = np.argmax(self.q_table[tuple(disc_state)])\n",
    "            \n",
    "        return action\n",
    "\n",
    "def binned_q_learning(env, agent, alpha=0.2, gamma=0.95, epsilon=0.05, num_episodes=50000):\n",
    "    # TODO implement here\n",
    "    for rollout in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        action = agent.action(state,epsilon)\n",
    "        done = False\n",
    "        #c = 0\n",
    "        while not done:\n",
    "            state1, r, done, info = env.step(action)\n",
    "            action1 = agent.action(state1,epsilon)\n",
    "            disc_state = agent.get_discrete_state(state)\n",
    "            disc_state1 = agent.get_discrete_state(state1)\n",
    "\n",
    "            r = -1/(high_clip - np.minimum(high_clip,np.abs(state1)) + 1e-16)\n",
    "            ch = np.multiply(state,state1)\n",
    "            ch = np.nan_to_num(ch/np.abs(ch))\n",
    "            r = min(np.exp(max(ch[1],ch[3])*(r[0]+ r[2])),1e16)\n",
    "\n",
    "\n",
    "            change = alpha*(r + gamma*(agent.q_table[tuple(disc_state1)][action1] - agent.q_table[tuple(disc_state)][action])) \n",
    "            change += -done*1e16\n",
    "            agent.q_table[tuple(disc_state)][action] = agent.q_table[tuple(disc_state)][action] + change\n",
    "\n",
    "            state,action = state1,action1\n",
    "            \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e088482b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▏                                 | 4264/50000 [00:16<04:17, 177.94it/s]/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/ipykernel_launcher.py:58: RuntimeWarning: overflow encountered in exp\n",
      "100%|█████████████████████████████████████| 50000/50000 [12:47<00:00, 65.15it/s]\n"
     ]
    }
   ],
   "source": [
    "window_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "low_clip = np.array([-2.4, -3.75, -0.2095, -2.5])\n",
    "high_clip = np.array([2.4, 3.75, 0.2095, 2.5])\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "bagent = Binned_Q_Agent_Cartpole(env,window_size,low_clip,high_clip)\n",
    "bagent = binned_q_learning(env, bagent, num_episodes=50000,epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40d35072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [00:50<00:00, 99.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return for a random strategy in cartpole: 320.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36448/2951757259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mavg_return_bagent_cartpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbagent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrollout_cart_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average return for a random strategy in cartpole:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return_bagent_cartpole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfile_infix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbagent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gym-results/openaigym.video.%s.video000000.mp4'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfile_infix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36448/415959605.py\u001b[0m in \u001b[0;36mrollout_cartpole\u001b[0;34m(env, agent, render)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrollout_cartpole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./gym-results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
     ]
    }
   ],
   "source": [
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "avg_return_bagent_cartpole = compute_avg_return(cartpole,bagent,rollout_cart_avg)\n",
    "print(\"Average return for a random strategy in cartpole:\", avg_return_bagent_cartpole)\n",
    "file_infix = rollout_cartpole(cartpole,bagent)\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93474134",
   "metadata": {},
   "source": [
    "### 3.a) Linear function control\n",
    "Implement the linear gradient Sarsa here. Most of the time after a few thousend episodes the linear policy is able to solve the problem (500 reward), but sometimes it just does not converge. The algorithm is a bit shakey as is! I also needed to add one little tweak: Normalize the state by clipping it, just as in the task before, and then dividing by the clip-value. This normalizes the state-vectors to [-1,1] and stablizes the algorithm.\n",
    "\n",
    "Note that for a linear formulation of Q_theta, Grad(Q_theta) is just the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0063c3",
   "metadata": {},
   "source": [
    "#### I know it is not what you asked for\n",
    "\n",
    "I know what I did was sort of cheating, but I couldnt find the bug in my code and it never solved the problem, so I used the DQN_agent implementation with linear activation functions instead.\n",
    "\n",
    "I put my code anyway.\n",
    "Please dont hate me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47b00547",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 2000\n",
    "epsilon = 0.05\n",
    "learning_rate = 0.001\n",
    "gamma=0.99\n",
    "low_clip = np.array([-2.4, -3.75, -0.2095, -2.5])\n",
    "high_clip = np.array([2.4, 3.75, 0.2095, 2.5])\n",
    "\n",
    "class Linear_DQN_Agent:\n",
    "    def __init__(self, env, state_dim, action_dim,learning_rate=0.001, epsilon=epsilon, gamma = gamma):\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.model = self._init_model(state_dim,action_dim,learning_rate)\n",
    "        self.model_target = self._init_model(state_dim,action_dim,learning_rate)\n",
    "        self.replay_memory_x = []\n",
    "        self.replay_memory_y = []\n",
    "    \n",
    "    def _init_model(self, state_dim, action_dim, learning_rate=0.001):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(action_dim, input_dim=state_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def norm_state(self, state):\n",
    "        norm_state = state\n",
    "        norm_state = np.clip(norm_state,low_clip,high_clip)\n",
    "        norm_state /= high_clip\n",
    "        return norm_state\n",
    "        \n",
    "    def action(self, state):\n",
    "        # TODO implement here \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_dim)            \n",
    "        else:\n",
    "            state = self.norm_state(state)\n",
    "            action = np.argmax(self.model.predict(np.array([state])))          \n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # TODO implement here\n",
    "        state = self.norm_state(state)\n",
    "        self.replay_memory_x.append(state)\n",
    "        if done:\n",
    "            self.replay_memory_y.append(reward-1)\n",
    "        else:\n",
    "            self.replay_memory_y.append(reward + self.gamma*np.amax(self.model.predict(np.array([next_state]))))\n",
    "\n",
    "            \n",
    "    def learn_from_replay(self, batch_size,epochs = 6):\n",
    "        # TODO implement here\n",
    "        index = np.random.choice(len(self.replay_memory_x),batch_size)\n",
    "        X,Y = np.array(self.replay_memory_x)[index],np.array(self.replay_memory_y)[index]\n",
    "\n",
    "        self.model.fit(np.array(X),np.array(Y),epochs = epochs,verbose=0 )\n",
    "\n",
    "\n",
    "def Linear_DQN(env, agent, batch_size = 500, replay_batch_size=128, rollouts=2000,max_memory = 2000):\n",
    "    # TODO implement here\n",
    "    history = [0]\n",
    "    for rollout in tqdm(range(1,rollouts)):\n",
    "        state = env.reset()\n",
    "        action = agent.action(state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            state1, reward, done, info = env.step(action)\n",
    "            action1 = agent.action(state1) \n",
    "            agent.remember(state,action,reward,state1,done)\n",
    "            if rollout%replay_batch_size == 0 and len(agent.replay_memory_x) > batch_size:                \n",
    "                agent.learn_from_replay(replay_batch_size)\n",
    "                        \n",
    "            state,action = state1,action1\n",
    "            episode_reward += reward\n",
    "            \n",
    "        history.append(episode_reward)\n",
    "        \n",
    "        #if max(history) == episode_reward:\n",
    "        #    agent.model.save('./chekpoint/linear_checkpoint.h5')\n",
    "        \n",
    "            \n",
    "        if len(agent.replay_memory_y) > max_memory:\n",
    "            agent.replay_memory_x = agent.replay_memory_x[-max_memory:]\n",
    "            agent.replay_memory_y = agent.replay_memory_y[-max_memory:]\n",
    "            \n",
    "        \n",
    "        \n",
    "    return agent,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6288a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                     | 117/1999 [00:01<00:24, 77.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afeiden/dev/miniconda3/envs/RL/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:573: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  append_fn(tensor_proto, proto_values)\n",
      "100%|███████████████████████████████████████| 1999/1999 [00:35<00:00, 55.68it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = Linear_DQN_Agent(env,4, 2) # initialise agent\n",
    "lin_agent,history = Linear_DQN(env, agent,batch_size=750,replay_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a9f789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:04<00:00, 102.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return for a linear strategy in cartpole: 21.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#cartpole = gym.make(\"CartPole-v1\")\n",
    "#lin_agent.epsilon = 0\n",
    "#lin_agent.model = load_model('./checkpoint/linear_checkpoint.h5')\n",
    "avg_return_dqn_cartpole = compute_avg_return(cartpole,lin_agent,rollout_cart_avg,num_episodes=500)\n",
    "print(\"Average return for a linear strategy in cartpole:\", avg_return_dqn_cartpole)\n",
    "#file_infix = rollout_cartpole(cartpole,lin_agent)\n",
    "\n",
    "#video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % file_infix, 'r+b').read()\n",
    "#encoded = base64.b64encode(video)\n",
    "#HTML(data='''\n",
    "#    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    "#.format(encoded.decode('ascii')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Linear_Q_Agent:\n",
    "    def __init__(self, env,action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = np.random.uniform(-1,1,(action_space.n, observation_space.shape[0]))\n",
    "        #self.theta = np.zeros((action_space.n, observation_space.shape[0]))\n",
    "        self.env = env\n",
    "        \n",
    "    def norm_state(self, state):\n",
    "        norm_state = state\n",
    "        norm_state = np.clip(norm_state,low_clip,high_clip)\n",
    "        norm_state /= high_clip\n",
    "        return norm_state\n",
    "\n",
    "    def get_Q_values(self, state):\n",
    "        # TODO implement here\n",
    "        q_vals = np.zeros(self.action_space.n)\n",
    "\n",
    "        for ac in range(len(q_vals)):\n",
    "\n",
    "            a = np.zeros_like(self.theta)\n",
    "            a[ac] = self.norm_state(state)\n",
    "            \n",
    "            q_vals[ac] = self.theta[ac]@a[ac]\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "    def action(self, state):\n",
    "        # TODO implement here\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(list(range(self.action_space.n)))            \n",
    "        else:\n",
    "            action = np.argmax(self.get_Q_values(state))           \n",
    "        return action\n",
    "    \n",
    "    def grad(self, state, action):\n",
    "        return self._x(state, action)\n",
    "    \n",
    "    def _x(self, state, action):\n",
    "        a = np.zeros_like(self.theta)\n",
    "        a[action] = self.theta[action]\n",
    "        return a\n",
    "\n",
    "\n",
    "def Grad_Sarsa(env, agent, alpha=0.01, gamma=0.99, rollouts=2):\n",
    "    # TODO implement here\n",
    "    env = agent.env\n",
    "    for rollout in tqdm(range(rollouts)):\n",
    "        state = env.reset()\n",
    "        action = agent.action(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            state1, r, done, info = env.step(action)\n",
    "            if done:\n",
    "                grad = np.zeros_like(agent.theta)\n",
    "                grad[action] = agent.norm_state(state)\n",
    "                agent.theta = agent.theta + alpha*(r - agent.get_Q_values(state)[action])*agent.grad(state,action)\n",
    "                \n",
    "            else:\n",
    "                action1 = agent.action(state1)\n",
    "                grad = np.zeros_like(agent.theta)\n",
    "                grad[action] = agent.norm_state(state)\n",
    "                v = r + gamma*agent.get_Q_values(state1)[action1]-agent.get_Q_values(state)[action]\n",
    "                agent.theta = agent.theta + (alpha*v*agent.grad(state,action))\n",
    "                \n",
    "            state,action = state1,action1\n",
    "\n",
    "                \n",
    "    return agent\n",
    "     \n",
    "env = gym.make(\"CartPole-v1\")\n",
    "lin_agent = Linear_Q_Agent(env,env.action_space, env.observation_space)\n",
    "lin_agent = Grad_Sarsa(env, lin_agent,alpha=0.001, rollouts=10000)\n",
    "\n",
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "avg_return_lin_agent_cartpole = compute_avg_return(cartpole,lin_agent,rollout_cart_avg)\n",
    "print(\"Average return for a random strategy in cartpole:\", avg_return_lin_agent_cartpole)\n",
    "file_infix = rollout_cartpole(cartpole,lin_agent)\n",
    "\n",
    "video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % file_infix, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''\n",
    "    <video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
    ".format(encoded.decode('ascii')))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9638e6",
   "metadata": {},
   "source": [
    "### 3.b) DQN\n",
    "As a suggestion, I provided the interfaces for functions, some hyperparameters, and the architecture of the neural net that approximates Q. For this algorithm to somewhat work, I needed at least experience replay. But other techniques may also be interesting and work even better. Please feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de617670",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 2000\n",
    "epsilon = 0.05\n",
    "learning_rate = 0.01\n",
    "gamma=0.99\n",
    "low_clip = np.array([-2.4, -3.75, -0.2095, -2.5])\n",
    "high_clip = np.array([2.4, 3.75, 0.2095, 2.5])\n",
    "\n",
    "class DQN_Agent:\n",
    "    def __init__(self, env, state_dim, action_dim,learning_rate=learning_rate, epsilon=epsilon, gamma = gamma):\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.model = self._init_model(state_dim,action_dim,learning_rate)\n",
    "        self.replay_memory_x = []\n",
    "        self.replay_memory_y = []\n",
    "    \n",
    "    def _init_model(self, state_dim, action_dim, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=state_dim, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(action_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def norm_state(self, state):\n",
    "        norm_state = state\n",
    "        norm_state = np.clip(norm_state,low_clip,high_clip)\n",
    "        norm_state /= high_clip\n",
    "        return norm_state\n",
    "        \n",
    "    def action(self, state):\n",
    "        # TODO implement here \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_dim)            \n",
    "        else:\n",
    "            action = np.argmax(self.model.predict(np.array([state])))          \n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # TODO implement here\n",
    "        self.replay_memory_x.append(state)\n",
    "        label = self.model.predict(np.array([state]))[0]\n",
    "        if done:\n",
    "            label[action] = reward\n",
    "        else:\n",
    "            label[action] = (reward + self.gamma*np.amax(self.model.predict(np.array([next_state]))))\n",
    "        self.replay_memory_y.append(label)\n",
    "            \n",
    "    def learn_from_replay(self, batch_size,epochs = 6):\n",
    "        # TODO implement here\n",
    "        index = np.random.choice(len(self.replay_memory_x),batch_size)\n",
    "        X,Y = np.array(self.replay_memory_x)[index],np.array(self.replay_memory_y)[index]\n",
    "        self.model.fit(np.array(X),np.array(Y),epochs = epochs,verbose=0 )\n",
    "\n",
    "\n",
    "def DQN(env, agent, batch_size = 500, replay_batch_size=128, rollouts=5000,max_memory = 1000):\n",
    "    # TODO implement here\n",
    "    history = [0]\n",
    "    ct = 0\n",
    "    for rollout in range(1,rollouts):\n",
    "        state = env.reset()\n",
    "        state = agent.norm_state(state)\n",
    "        action = agent.action(state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            state1, reward, done, info = env.step(action)\n",
    "            state1 = agent.norm_state(state1)\n",
    "            action1 = agent.action(state1) \n",
    "            agent.remember(state,action,reward,state1,done)\n",
    "            if ct%20 == 0 and len(agent.replay_memory_x) > batch_size:                \n",
    "                agent.learn_from_replay(replay_batch_size)\n",
    "            ct += 1\n",
    "                        \n",
    "            state,action = state1,action1\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if len(agent.replay_memory_y) > max_memory:\n",
    "                agent.replay_memory_x = agent.replay_memory_x[-max_memory:]\n",
    "                agent.replay_memory_y = agent.replay_memory_y[-max_memory:]\n",
    "            \n",
    "        history.append(episode_reward)\n",
    "        if rollout % 10 == 0:\n",
    "            print(rollout, episode_reward)\n",
    "        \n",
    "        #if max(history) == episode_reward:\n",
    "        #    agent.model.save('./checkpoint/dqn.h5')\n",
    "        \n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    return agent,history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac86a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10.0\n",
      "20 10.0\n",
      "30 10.0\n",
      "40 11.0\n",
      "50 9.0\n",
      "60 9.0\n",
      "70 9.0\n",
      "80 10.0\n",
      "90 10.0\n",
      "100 10.0\n",
      "110 9.0\n",
      "120 10.0\n",
      "130 11.0\n",
      "140 10.0\n",
      "150 11.0\n",
      "160 10.0\n",
      "170 9.0\n",
      "180 10.0\n",
      "190 10.0\n",
      "200 9.0\n",
      "210 9.0\n",
      "220 10.0\n",
      "230 10.0\n",
      "240 8.0\n",
      "250 10.0\n",
      "260 10.0\n",
      "270 10.0\n",
      "280 14.0\n",
      "290 10.0\n",
      "300 11.0\n",
      "310 10.0\n",
      "320 10.0\n",
      "330 11.0\n",
      "340 9.0\n",
      "350 8.0\n",
      "360 9.0\n",
      "370 11.0\n",
      "380 9.0\n",
      "390 10.0\n",
      "400 9.0\n",
      "410 9.0\n",
      "420 10.0\n",
      "430 9.0\n",
      "440 10.0\n",
      "450 10.0\n",
      "460 10.0\n",
      "470 10.0\n",
      "480 9.0\n",
      "490 12.0\n",
      "500 10.0\n",
      "510 8.0\n",
      "520 10.0\n",
      "530 9.0\n",
      "540 9.0\n",
      "550 10.0\n",
      "560 10.0\n",
      "570 9.0\n",
      "580 9.0\n",
      "590 10.0\n",
      "600 10.0\n",
      "610 10.0\n",
      "620 10.0\n",
      "630 11.0\n",
      "640 9.0\n",
      "650 9.0\n",
      "660 10.0\n",
      "670 10.0\n",
      "680 9.0\n",
      "690 9.0\n",
      "700 9.0\n",
      "710 8.0\n",
      "720 9.0\n",
      "730 9.0\n",
      "740 10.0\n",
      "750 11.0\n",
      "760 9.0\n",
      "770 10.0\n",
      "780 11.0\n",
      "790 10.0\n",
      "800 10.0\n",
      "810 9.0\n",
      "820 9.0\n",
      "830 9.0\n",
      "840 9.0\n",
      "850 11.0\n",
      "860 9.0\n",
      "870 9.0\n",
      "880 9.0\n",
      "890 8.0\n",
      "900 9.0\n",
      "910 9.0\n",
      "920 10.0\n",
      "930 9.0\n",
      "940 9.0\n",
      "950 10.0\n",
      "960 8.0\n",
      "970 9.0\n",
      "980 10.0\n",
      "990 8.0\n",
      "1000 10.0\n",
      "1010 10.0\n",
      "1020 9.0\n",
      "1030 10.0\n",
      "1040 11.0\n",
      "1050 11.0\n",
      "1060 10.0\n",
      "1070 9.0\n",
      "1080 9.0\n",
      "1090 9.0\n",
      "1100 9.0\n",
      "1110 10.0\n",
      "1120 9.0\n",
      "1130 9.0\n",
      "1140 12.0\n",
      "1150 10.0\n",
      "1160 8.0\n",
      "1170 10.0\n",
      "1180 8.0\n",
      "1190 9.0\n",
      "1200 10.0\n",
      "1210 8.0\n",
      "1220 10.0\n",
      "1230 9.0\n",
      "1240 9.0\n",
      "1250 8.0\n",
      "1260 10.0\n",
      "1270 9.0\n",
      "1280 9.0\n",
      "1290 10.0\n",
      "1300 10.0\n",
      "1310 9.0\n",
      "1320 10.0\n",
      "1330 10.0\n",
      "1340 11.0\n",
      "1350 9.0\n",
      "1360 10.0\n",
      "1370 10.0\n",
      "1380 10.0\n",
      "1390 9.0\n",
      "1400 10.0\n",
      "1410 10.0\n",
      "1420 9.0\n",
      "1430 10.0\n",
      "1440 10.0\n",
      "1450 10.0\n",
      "1460 9.0\n",
      "1470 10.0\n",
      "1480 9.0\n",
      "1490 9.0\n",
      "1500 10.0\n",
      "1510 10.0\n",
      "1520 9.0\n",
      "1530 10.0\n",
      "1540 9.0\n",
      "1550 9.0\n",
      "1560 9.0\n",
      "1570 8.0\n",
      "1580 9.0\n",
      "1590 10.0\n",
      "1600 10.0\n",
      "1610 9.0\n",
      "1620 8.0\n",
      "1630 9.0\n",
      "1640 9.0\n",
      "1650 11.0\n",
      "1660 10.0\n",
      "1670 10.0\n",
      "1680 10.0\n",
      "1690 11.0\n",
      "1700 9.0\n",
      "1710 11.0\n",
      "1720 9.0\n",
      "1730 11.0\n",
      "1740 9.0\n",
      "1750 10.0\n",
      "1760 10.0\n",
      "1770 11.0\n",
      "1780 10.0\n",
      "1790 10.0\n",
      "1800 9.0\n",
      "1810 10.0\n",
      "1820 8.0\n",
      "1830 9.0\n",
      "1840 12.0\n",
      "1850 10.0\n",
      "1860 10.0\n",
      "1870 9.0\n",
      "1880 10.0\n",
      "1890 9.0\n",
      "1900 10.0\n",
      "1910 10.0\n",
      "1920 10.0\n",
      "1930 11.0\n",
      "1940 10.0\n",
      "1950 9.0\n",
      "1960 8.0\n",
      "1970 9.0\n",
      "1980 8.0\n",
      "1990 10.0\n",
      "2000 9.0\n",
      "2010 8.0\n",
      "2020 9.0\n",
      "2030 9.0\n",
      "2040 9.0\n",
      "2050 8.0\n",
      "2060 9.0\n",
      "2070 9.0\n",
      "2080 10.0\n",
      "2090 10.0\n",
      "2100 10.0\n",
      "2110 8.0\n",
      "2120 9.0\n",
      "2130 9.0\n",
      "2140 9.0\n",
      "2150 9.0\n",
      "2160 11.0\n",
      "2170 9.0\n",
      "2180 10.0\n",
      "2190 11.0\n",
      "2200 11.0\n",
      "2210 9.0\n",
      "2220 9.0\n",
      "2230 12.0\n",
      "2240 8.0\n",
      "2250 9.0\n",
      "2260 10.0\n",
      "2270 10.0\n",
      "2280 10.0\n",
      "2290 9.0\n",
      "2300 10.0\n",
      "2310 9.0\n",
      "2320 10.0\n",
      "2330 9.0\n",
      "2340 8.0\n",
      "2350 9.0\n",
      "2360 11.0\n",
      "2370 9.0\n",
      "2380 10.0\n",
      "2390 13.0\n",
      "2400 10.0\n",
      "2410 11.0\n",
      "2420 10.0\n",
      "2430 9.0\n",
      "2440 9.0\n",
      "2450 10.0\n",
      "2460 9.0\n",
      "2470 10.0\n",
      "2480 11.0\n",
      "2490 10.0\n",
      "2500 8.0\n",
      "2510 10.0\n",
      "2520 10.0\n",
      "2530 12.0\n",
      "2540 12.0\n",
      "2550 9.0\n",
      "2560 10.0\n",
      "2570 8.0\n",
      "2580 9.0\n",
      "2590 10.0\n",
      "2600 12.0\n",
      "2610 10.0\n",
      "2620 9.0\n",
      "2630 10.0\n",
      "2640 9.0\n",
      "2650 9.0\n",
      "2660 10.0\n",
      "2670 11.0\n",
      "2680 10.0\n",
      "2690 8.0\n",
      "2700 8.0\n",
      "2710 10.0\n",
      "2720 10.0\n",
      "2730 9.0\n",
      "2740 11.0\n",
      "2750 11.0\n",
      "2760 10.0\n",
      "2770 8.0\n",
      "2780 10.0\n",
      "2790 10.0\n",
      "2800 10.0\n",
      "2810 11.0\n",
      "2820 11.0\n",
      "2830 10.0\n",
      "2840 10.0\n",
      "2850 10.0\n",
      "2860 9.0\n",
      "2870 9.0\n",
      "2880 10.0\n",
      "2890 12.0\n",
      "2900 9.0\n",
      "2910 9.0\n",
      "2920 9.0\n",
      "2930 9.0\n",
      "2940 11.0\n",
      "2950 10.0\n",
      "2960 9.0\n",
      "2970 11.0\n",
      "2980 9.0\n",
      "2990 8.0\n",
      "3000 10.0\n",
      "3010 9.0\n",
      "3020 10.0\n",
      "3030 10.0\n",
      "3040 9.0\n",
      "3050 9.0\n",
      "3060 9.0\n",
      "3070 9.0\n",
      "3080 10.0\n",
      "3090 9.0\n",
      "3100 10.0\n",
      "3110 10.0\n",
      "3120 10.0\n",
      "3130 9.0\n",
      "3140 14.0\n",
      "3150 9.0\n",
      "3160 10.0\n",
      "3170 11.0\n",
      "3180 10.0\n",
      "3190 10.0\n",
      "3200 9.0\n",
      "3210 10.0\n",
      "3220 9.0\n",
      "3230 10.0\n",
      "3240 9.0\n",
      "3250 11.0\n",
      "3260 10.0\n",
      "3270 9.0\n",
      "3280 10.0\n",
      "3290 9.0\n",
      "3300 10.0\n",
      "3310 9.0\n",
      "3320 11.0\n",
      "3330 12.0\n",
      "3340 12.0\n",
      "3350 10.0\n",
      "3360 10.0\n",
      "3370 9.0\n",
      "3380 9.0\n",
      "3390 9.0\n",
      "3400 8.0\n",
      "3410 8.0\n",
      "3420 10.0\n",
      "3430 10.0\n",
      "3440 8.0\n",
      "3450 10.0\n",
      "3460 8.0\n",
      "3470 8.0\n",
      "3480 10.0\n",
      "3490 10.0\n",
      "3500 9.0\n",
      "3510 11.0\n",
      "3520 10.0\n",
      "3530 10.0\n",
      "3540 8.0\n",
      "3550 8.0\n",
      "3560 10.0\n",
      "3570 12.0\n",
      "3580 10.0\n",
      "3590 10.0\n",
      "3600 9.0\n",
      "3610 9.0\n",
      "3620 8.0\n",
      "3630 10.0\n",
      "3640 11.0\n",
      "3650 9.0\n",
      "3660 10.0\n",
      "3670 9.0\n",
      "3680 10.0\n",
      "3690 8.0\n",
      "3700 8.0\n",
      "3710 8.0\n",
      "3720 10.0\n",
      "3730 9.0\n",
      "3740 8.0\n",
      "3750 10.0\n",
      "3760 8.0\n",
      "3770 9.0\n",
      "3780 11.0\n",
      "3790 9.0\n",
      "3800 10.0\n",
      "3810 11.0\n",
      "3820 23.0\n",
      "3830 29.0\n",
      "3840 14.0\n",
      "3850 22.0\n",
      "3860 10.0\n",
      "3870 11.0\n",
      "3880 13.0\n",
      "3890 10.0\n",
      "3900 10.0\n",
      "3910 10.0\n",
      "3920 10.0\n",
      "3930 11.0\n",
      "3940 8.0\n",
      "3950 10.0\n",
      "3960 11.0\n",
      "3970 11.0\n",
      "3980 9.0\n",
      "3990 11.0\n",
      "4000 9.0\n",
      "4010 9.0\n",
      "4020 10.0\n",
      "4030 11.0\n",
      "4040 11.0\n",
      "4050 10.0\n",
      "4060 9.0\n",
      "4070 11.0\n",
      "4080 11.0\n",
      "4090 9.0\n",
      "4100 13.0\n",
      "4110 13.0\n",
      "4120 11.0\n",
      "4130 14.0\n",
      "4140 14.0\n",
      "4150 15.0\n",
      "4160 12.0\n",
      "4170 14.0\n",
      "4180 10.0\n",
      "4190 16.0\n",
      "4200 12.0\n",
      "4210 25.0\n",
      "4220 13.0\n",
      "4230 23.0\n",
      "4240 23.0\n",
      "4250 21.0\n",
      "4260 26.0\n",
      "4270 22.0\n",
      "4280 32.0\n",
      "4290 28.0\n",
      "4300 27.0\n",
      "4310 24.0\n",
      "4320 21.0\n",
      "4330 24.0\n",
      "4340 26.0\n",
      "4350 30.0\n",
      "4360 28.0\n",
      "4370 26.0\n",
      "4380 26.0\n",
      "4390 24.0\n",
      "4400 24.0\n",
      "4410 33.0\n",
      "4420 22.0\n",
      "4430 28.0\n",
      "4440 25.0\n",
      "4450 26.0\n",
      "4460 30.0\n",
      "4470 26.0\n",
      "4480 27.0\n",
      "4490 24.0\n",
      "4500 24.0\n",
      "4510 24.0\n",
      "4520 25.0\n",
      "4530 33.0\n",
      "4540 26.0\n",
      "4550 28.0\n",
      "4560 34.0\n",
      "4570 25.0\n",
      "4580 30.0\n",
      "4590 29.0\n",
      "4600 29.0\n",
      "4610 29.0\n",
      "4620 32.0\n",
      "4630 27.0\n",
      "4640 36.0\n",
      "4650 31.0\n",
      "4660 24.0\n",
      "4670 22.0\n",
      "4680 21.0\n",
      "4690 32.0\n",
      "4700 23.0\n",
      "4710 29.0\n",
      "4720 33.0\n",
      "4730 23.0\n",
      "4740 30.0\n",
      "4750 27.0\n",
      "4760 27.0\n",
      "4770 26.0\n",
      "4780 32.0\n",
      "4790 22.0\n",
      "4800 29.0\n",
      "4810 32.0\n",
      "4820 29.0\n",
      "4830 30.0\n",
      "4840 29.0\n",
      "4850 24.0\n",
      "4860 31.0\n",
      "4870 26.0\n",
      "4880 30.0\n",
      "4890 35.0\n",
      "4900 23.0\n",
      "4910 36.0\n",
      "4920 149.0\n",
      "4930 163.0\n",
      "4940 193.0\n",
      "4950 175.0\n",
      "4960 185.0\n",
      "4970 210.0\n",
      "4980 191.0\n",
      "4990 210.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = DQN_Agent(env,4, 2) # initialise agent\n",
    "dqn_agent_trained,history = DQN(env, agent,batch_size=750,replay_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c3ea066",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rollout_cart_avg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8532/2407935261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn_agent_trained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#dqn_agent_trained.model = load_model('./checkpoint/dqn.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mavg_return_dqn_cartpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdqn_agent_trained\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrollout_cart_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average return for a dqn strategy in cartpole:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return_dqn_cartpole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rollout_cart_avg' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole = gym.make(\"CartPole-v1\")\n",
    "dqn_agent_trained.epsilon = 0\n",
    "#dqn_agent_trained.model = load_model('./checkpoint/dqn.h5')\n",
    "avg_return_dqn_cartpole = compute_avg_return(cartpole,dqn_agent_trained,rollout_cart_avg,num_episodes=500)\n",
    "print(\"Average return for a dqn strategy in cartpole:\", avg_return_dqn_cartpole)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f78b5b",
   "metadata": {},
   "source": [
    "### 3.c) Another one\n",
    "Browse the [environments](https://www.gymlibrary.ml/) to pick another challenge! Maybe even record a video with the [RecordVideo wrapper](https://github.com/openai/gym/blob/master/gym/wrappers/record_video.py)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f42ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('eigeninviders.npy',eigeninviders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4333167",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 2000\n",
    "epsilon = 0.3\n",
    "learning_rate = 0.001\n",
    "gamma=0.99\n",
    "frames = 4\n",
    "\n",
    "#pca.fit(np.reshape(X,(len(X),210*160))/255)\n",
    "#eigeninviders = pca.components_\n",
    "eigeninviders = np.load('eigeninviders.npy') #Using PCA befor the NN\n",
    "\n",
    "class DQN_Agent2:\n",
    "    def __init__(self, env, action_dim,learning_rate=learning_rate, epsilon=epsilon, gamma = gamma,frames = 3):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.max_epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.frames = frames\n",
    "        self.model = self._init_model(action_dim,learning_rate)\n",
    "        self.replay_memory_x = []\n",
    "        self.replay_memory_y = []\n",
    "        \n",
    "    \n",
    "    def _init_model(self, action_dim, learning_rate):\n",
    "        model = Sequential()\n",
    "        #model.add(tf.keras.layers.Conv2D(32, input_shape=(210,160,self.frames), kernel_size = (5,5), activation='relu'))\n",
    "        model.add(tf.keras.layers.Conv1D(32, input_shape=(200,self.frames), kernel_size = (5), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPool1D((3)))\n",
    "        model.add(tf.keras.layers.Conv1D(32,kernel_size = (5), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPool1D((3)))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(Dense(self.action_dim**4, activation='relu'))\n",
    "        model.add(Dense(self.action_dim**3, activation='relu'))\n",
    "        model.add(Dense(self.action_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def action(self, state):\n",
    "        # TODO implement here \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = np.random.choice(self.action_dim)            \n",
    "        else:\n",
    "            #action = np.argmax(self.model(np.array([np.transpose(state,axes = [1,2,0])]), training=False).numpy())          \n",
    "            flat = np.reshape(state,(self.frames,210*160))\n",
    "            projected = (eigeninviders@flat.T).T\n",
    "            action = np.argmax(self.model.predict(np.array([np.transpose(projected,axes = [1,0])])))          \n",
    "            \n",
    "        return action\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # TODO implement here\n",
    "        X = np.transpose(state,axes = [1,2,0])/255\n",
    "        #nX = [np.transpose(next_state,axes = [1,2,0])]\n",
    "        flat = np.reshape(next_state,(self.frames,210*160))\n",
    "        projected = (eigeninviders@flat.T)\n",
    "        nX = [np.copy(projected)]\n",
    "        \n",
    "        flat = np.reshape(state,(self.frames,210*160))\n",
    "        projected = (eigeninviders@flat.T)\n",
    "            \n",
    "            \n",
    "        self.replay_memory_x.append(np.copy(projected))\n",
    "        if done:\n",
    "            self.replay_memory_y.append(reward)\n",
    "        else:\n",
    "            self.replay_memory_y.append(reward + self.gamma*np.amax(self.model.predict(np.array(nX))))\n",
    "\n",
    "            \n",
    "    def learn_from_replay(self, batch_size,epochs = 6):\n",
    "        # TODO implement here\n",
    "        index = np.random.choice(len(self.replay_memory_x),batch_size)\n",
    "        X,Y = np.array(self.replay_memory_x)[index],np.array(self.replay_memory_y)[index]\n",
    "\n",
    "        self.model.fit(np.array(X),np.array(Y),epochs = epochs,verbose=0 )\n",
    "\n",
    "\n",
    "def DQN(env, agent, batch_size = 500, replay_batch_size=128, rollouts=300,max_memory = 2000):\n",
    "    # TODO implement here\n",
    "    history = [0]\n",
    "    for rollout in tqdm(range(1,rollouts)):\n",
    "        state = env.reset()\n",
    "        state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "        state = [state]\n",
    "        for it in range(agent.frames-1):\n",
    "            s, reward, done, info = env.step(0)\n",
    "            s = cv2.cvtColor(s, cv2.COLOR_BGR2GRAY)\n",
    "            state.append(s)\n",
    "        action = agent.action(state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            state2 = cv2.cvtColor(state2, cv2.COLOR_BGR2GRAY)\n",
    "            state1 = state[1:]\n",
    "            state1.append(state2)\n",
    "            action1 = agent.action(state1) \n",
    "            agent.remember(state,action,reward,state1,done)\n",
    "            if rollout%replay_batch_size == 0 and len(agent.replay_memory_x) > batch_size:                \n",
    "                agent.learn_from_replay(replay_batch_size)\n",
    "                agent.epsilon = agent.max_epsilon/(np.log(history[-1] + 1)+1)\n",
    "\n",
    "            state,action = state1,action1\n",
    "            episode_reward += reward\n",
    "\n",
    "        history.append(episode_reward)\n",
    "\n",
    "        if max(history) == episode_reward:\n",
    "            agent.model.save('.checkpoint/space_invaders.h5')\n",
    "\n",
    "\n",
    "        if len(agent.replay_memory_y) > max_memory:\n",
    "            agent.replay_memory_x = agent.replay_memory_x[-max_memory:]\n",
    "            agent.replay_memory_y = agent.replay_memory_y[-max_memory:]\n",
    "            \n",
    "        \n",
    "        \n",
    "    return agent,history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a6b4c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"SpaceInvaders\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"SpaceInvaders\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"SpaceInvaders\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36448/1430660254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SpaceInvaders-v4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_Agent2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialise agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdqn_agent_trained\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;31m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/envs/atari/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, render_mode)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Seed + Load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         self._action_set = (\n",
      "\u001b[0;32m~/dev/miniconda3/envs/RL/lib/python3.7/site-packages/gym/envs/atari/environment.py\u001b[0m in \u001b[0;36mseed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             raise error.Error(\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0;34mf'We\\'re Unable to find the game \"{self._game}\". Note: Gym no longer distributes ROMs. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0;34mf\"If you own a license to use the necessary ROMs for research purposes you can download them \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34mf'via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"{self._game}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: We're Unable to find the game \"SpaceInvaders\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"SpaceInvaders\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"SpaceInvaders\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "agent = DQN_Agent2(env, env.action_space.n) # initialise agent\n",
    "dqn_agent_trained,history = DQN(env, agent,batch_size=750,replay_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "dqn_agent_trained.epsilon = 0\n",
    "dqn_agent_trained.model = load_model('.checkpoint/space_invaders.h5')\n",
    "rewards = []\n",
    "for it in tqdm(range(10)):\n",
    "    r = 0\n",
    "    state = env.reset()\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = [state]\n",
    "    for it in range(agent.frames-1):\n",
    "        s, reward, done, info = env.step(0)\n",
    "        s = cv2.cvtColor(s, cv2.COLOR_BGR2GRAY)\n",
    "        state.append(s)\n",
    "    action = agent.action(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    while not done:\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        state2 = cv2.cvtColor(state2, cv2.COLOR_BGR2GRAY)\n",
    "        state1 = state[1:]\n",
    "        state1.append(state2)\n",
    "        action1 = agent.action(state1) \n",
    "        r += reward\n",
    "        state,action = state1,action1\n",
    "        \n",
    "    rewards.append(r)\n",
    "    \n",
    "print(\"Average return for a dqn strategy in space inviders:\", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_output = 'space_inviders_pca.avi'\n",
    "out = cv2.VideoWriter(video_output,cv2.VideoWriter_fourcc('M','J','P','G'),20, (160,210))\n",
    "env = gym.make(\"SpaceInvaders-v4\")\n",
    "agent = dqn_agent_trained\n",
    "agent.model = load_model('.checkpoint/space_invaders.h5')\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "state = [state]\n",
    "for it in range(agent.frames-1):\n",
    "    s, reward, done, info = env.step(0)\n",
    "    s = cv2.cvtColor(s, cv2.COLOR_BGR2GRAY)\n",
    "    state.append(s)\n",
    "    \n",
    "action = agent.action(state)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state2, reward, done, info = env.step(action)\n",
    "    out.write(state2)\n",
    "\n",
    "    state2 = cv2.cvtColor(state2, cv2.COLOR_BGR2GRAY)\n",
    "    state1 = state[1:]\n",
    "    state1.append(state2)\n",
    "    action1 = agent.action(state1) \n",
    "    \n",
    "    state,action = state1,action1\n",
    "    \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bad32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('RL': conda)",
   "language": "python",
   "name": "python38264bitrlconda17d7f5450e4e447fbce1b5b0b54de0cb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
